[[rg-landscape-standard-section]]
== OGC GeoPose in Context

In this section of the reviewer’s guide, the Standards Development Organizations (SDOs) and standards that specify data objects that convey the same information as one or more of the eight forms of GeoPose are listed.
As a convention for the remainder of this section, the term "GeoPose" is used as a shorthand for any of the specified forms of the OGC GeoPose Standard.

The key question this section answers is where and how GeoPose fits into the current landscape of standards. By examining some of the relevant standards, this section illustrates the gaps which GeoPose fills.

There are many existing and emerging standards for position and orientation information. They have emerged from requirements defined in different industries: Aviation, planetary sciences, maritime, robotics, autonomous driving, satellite positioning, aerospace, and so forth. There is good practice in commercial and other domains for expressing the position and orientation of entities in all these fields. These existing standards cover different scales, for different purposes, different information environment - sometimes graphical, sometimes geospatial.

=== OGC GeoPose Connects Objects to their Representation
There are both conceptual and actual data pipelines that connect the real world and the objects in it with the representations in information and graphics systems. A number of the critical standards define how those pipelines are developed and interoperate or fit together.

In a standard Web browser, information is displayed on a plane. There is a need for a standard to represent information retrieved from the Web on the 2D display plane in a manner that is sufficiently fast to provide XR experiences. https://www.w3.org/TR/webxr/[W3C WebXR] focuses on this need.

In order to establish the data flow required for 2D to XR experiences, there is a need in graphics systems to locate those objects within a 3D local stage (or scene) so that when the user’s head or perceived objects move within that “stage”, the graphics systems can address those movements and changes correctly. https://www.khronos.org/openxr/[Khronos OpenXR] focuses on that stage of the pipeline.

In parallel, the OGC has been working on how sensor systems measure these changes in position and orientation. How does a system model the sensor that is capturing all the data about the features (e.g., objects) in its environment, including where they are and how the system represents them in a local 2D or 3D environment? OGC Sensor Web Enablement, particularly the OGC Abstract Specification Topic 20: Observations Measurements and Samples and SensorML Standards, addresses these needs.

Each of these stages in the pipeline needs to exchange data between themselves. How do you get the position and orientation of anything from anywhere to anywhere, into and out of these interactive environments? That’s where GeoPose comes in. GeoPose defines the data structure(s) to pass position and orientation information between elements in the pipeline.

=== GeoPose for 3D Asset Formats

The world of 3D asset formats is highly complex. New formats are continuously being developed as new modeling and content creation applications and 3D content distribution platforms appear. This problem increases costs in 3D asset pipelines as well as leaving lots of stakeholders frustrated. The need for standardization is apparent.

Today, two complementary 3D asset formats are becoming widely adopted. The Khronos Group's glTF open standard is a transmission format to enable the efficient and reliable distribution of 3D assets to a wide variety of platforms including both Web and native applications. The open source USD format published by Pixar is an authoring format that enables teams of designers to collaborate on the creation of 3D assets.

In the spatial computing industry and, in particular, for use cases handling 3D assets in real-world anchored scenes, there is not (to the knowledge of this working group) a standardized solution for encoding a real world pose of 3D assets. Stakeholders who either work with or develop proprietary 3D formats or standards for 3D asset formats, should consider creating extensions that comply with the OGC GeoPose Standard to their formats.

==== The Khronos Group's glTF format and GeoPose

The Khronos Group is an important standards organization developing 3D and XR run-time APIs and 3D assets formats, including OpenGL, WebGL, Vulkan, OpenXR and glTF. glTF 2.0 enjoys support by a wide community and growing number of tools, engines and applications and has been submitted to ISO/IEC JTC 1 to become an international standard.

An extension to include GeoPose in glTF would be an important step for interoperability between the 3D computer graphics and geospatial industries.

==== Pixar's open-source USD format and GeoPose

Pixar, the company responsible for a long list of blockbuster 3D animated movies from the 1995 "Toy Story" to the upcoming feature film "Turning Red," created the whole genre of full-length 3D animated movies while creating an impressive number of technology breakthroughs in the fields of 3D graphics rendering and animation. Pixar engineers then unified and standardized the way different systems used by different people inside their organization could work on the same highly complex scene without the hassle of converting to and from different file formats and importing and exporting them between different systems. Pixar developers came up with the https://graphics.pixar.com/usd/release/index.html[Universal Scene Description] format (abbreviated USD). Realizing in 2016, that the solution to their internal interoperability issues would potentially solve industry-wide interoperability problems of 3D graphics and animation, they https://graphics.pixar.com/usd/release/press_opensource_release.html[open sourced] their code and documentation for the format.  In 2018, Apple decided to use a zip-archive version of USD for 3D assets used in AR applications running on the iOS platforms. In 2019, NVIDIA announced they would standardize on USD for their Omniverse platform. Omniverse is being adopted for a range of industries such as autonomous vehicles, robotics, digital twins and city planning.  In 2021, support for USD export and import was added to a range of 3D authoring software applications such as 3DS Max, Maya and Blender.

Although USD is not specified by a traditional standards body, it is developing into a de-facto authoring standard with wide adoption across a wide range of industries, and the wider tech world can contribute through the https://github.com/PixarAnimationStudios/USD[USD repo on GitHub].

Just like with the glTF format, USD allows for the creation of extensions. One potential avenue for the GeoPose SWG to explore would be to propose a collaboration with NVIDIA. NVIDIA has already gained experience creating extensions for USD and has a lot of industry customers who work in the geospatial domain who would be likely to find such an extension useful.

=== OGC GeoPose in the Landscape of Standards
The primary source of inspiration for GeoPose was the NASA SPICE framework. This framework is able to cover any scale from interplanetary and interstellar to specific local objects. NASA designed SPICE to address significant challenges in looking at both ephemeris objects (fixed, or on predictable paths) and objects that have changeable positions and orientations such as satellites, cameras, and other sensors. Representing different frames for these objects and being able to transform between them is really useful. SPICE is a formalism that is much larger than required for a simple or basic implementation, but is an incredibly appropriate foundation from which GeoPose evolved.

When GeoPose was designed, the OGC Moving Features Standard was considered. Although Moving Features describes object position and orientation, that standard is focused on a particular set of use cases with an emphasis on sensor streams, digital exhausts, and location information (usually GPS) coming off of vehicles in a municipal environment. Moving Features accomplishes this compactly so that the data can be easily incorporated into analytical and visual applications.

OGC Moving Features and GeoPose have distinct roles and are complementary. Moving Features is focused on a local, municipal scale, and rapid streams of measurements. Getting observations in and out of municipal management and other platforms easily and efficiently is one of the roles that GeoPose plays.

The OGC Sensor Web Enablement (SWE) suite of standards deals with how to work with sensors and requesting observations in a standard encoding.

.OGC GeoPose and Sensor Web Enablement
image::landscape-standard-ef0fe.png["GeoPose and SWE"  pdfwidth="14cm"]

On the right of the above figure is a schematic showing the workflows that are enabled in Observations and Measurements, in Semantic Sensor Networks, and with encodings such as SWE Common that enable transport of sensor outputs (observations). Another important part of these standards, SensorML, models the sensor process itself, from an initial environmental stimulus to how a measurement is recorded as an observation and recognized as an observed property of a real world object.

If sampling from a directed sensor (e.g., camera), orientation is an essential aspect of the sensor model. The result potentially includes the positions and orientations of both the sensor / platform and any observed entities in the world. However, the sensor position and orientation may be secondary to the primary objective of observing the positions and orientations of these observed entities, such as cars. GeoPose permits systems to get the position and orientation in and out of SWE-compliant platforms or devices without losing any resolution or introducing delays.

Pose is essential for combining physical and digital entities in visual scenes for them to be used by a service or a user, particularly if entities are being brought into a scene from very different source contexts, and regardless of where they are in space.

Khronos OpenXR also handles poses, particularly within specific spaces (frames of reference). It defines a set of reference spaces (view, local and stage); and specifies the model in which graphics hardware can use the pose in rendering objects. Within a particular graphical system, it is effective but GeoPose adds the capability to bring in a pose from any source in any frame of reference.

GeoPose can also relate the frame of reference of a Web browser window to a virtual world or the real world.

Geocentric (Earth-based) position and orientation are the basis for all these integrations. OGC GeoPose provides that usable common ground, both the geospatial expertise that OGC has cultivated for many years and digital representation of physical space as the most common denominator among all these systems and representations.

To summarize, there are a number of well-developed standards for position and orientation. What these lack is a means for position and orientation information to be passed between them in a manner that is independent of graphical system, applications scene, frame of reference, and technology. OGC GeoPose offers portability of information between all these domains and systems.

The approaches to this issue that have been published in other standards prior to introduction of GeoPose appear in the tables below.

=== Standards which Could Reference GeoPose in Normative Clauses

==== OGC Standards

The OGC has many standards that express requirements for positioning and location. Some also express orientation. They do so in different scales and with different global and/or local coordinate reference systems. Some also deal with different time scales. However, these standards are not designed for sharing both position and orientation.

Some OGC standards (such as CDB) deal with fixed infrastructure. Other OGC standards, such as KML and IndoorGML, deal with somewhat more specialized information. Yet other OGC standards deal with expressing location and orientation in very dynamic and real time scales, such as Sensor Web Enablement and Moving Features.

The GeoPose SWG's assessment of the pose-related elements of the OGC standards is summarized in the table below with the following headings:

 - *Graphic/Virtual Context*: Does the standard relate only to a computer graphics context, abstracted from the outside world, or does the standard deal with virtual models of the real world?
 - *Local SRS*: Does the standard allow a local spatial reference system (SRS), independent of a wider geographical framework, to be used to define coordinates?
 - *Geodetic CRS*: Does the standard allow a spatial reference system connected to the shape of the Earth or its gravity field as the basis for coordinates?
 - *6DOF as entity or attributes*: If the standard stores position and orientation for objects, is this information stored as attributes of the objects or does it use a pose entity in the data model with an association between the pose entity and the object entity?
 - *Temporality*: Does the standard manage temporal information for the objects represented?

[%header,cols="3,2,1,2,2,2,2"]
|===
|Standard|Graphic/Virtual Context|Local SRS|Geodetic CRS|6DOF as entity or attributes?|Temporality|Remark

|link:https://www.ogc.org/standards/movingfeatures[Moving Features]|Virtual|Y|Y|Attributes of temporal geometry|Y|

|link:https://www.ogc.org/standards/swecommon[Sensor Web Enablement (SWE)]|Virtual|Y|Y|Attributes|Y|

|link:https://www.ogc.org/standards/gml[GML] |Virtual|Y|Y|Attributes. Direction only (no roll)|Y|v3.2.1

|link:https://www.ogc.org/standards/citygml[CityGML] |Both|Y|Y|Attributes|Y|

|link:https://www.ogc.org/standards/indoorgml[IndoorGML] |Virtual|Y|Y|No orientation|IndoorPOI extension|Based on GML

|link:https://www.ogc.org/standards/cdb[CDB] |Both|Y|Y|Y|Y|

|link:https://www.ogc.org/standards/kml[KML] |Both|Y|Y|Entity|Y|

|link:https://www.ogc.org/standards/om[Observations, Measurements and Samples (OMS)]|Both|Y|Y|Both|Y|

|link:https://www.ogc.org/standards/sensorthings[SensorThings API]|Virtual|Y|Y|Attributes|Y|

|link:https://docs.ogc.org/cs/20-094/index.html[Indoor Mapping Data Format (IMDF)]|Both|Y|Y|Attributes|Y|

|link:https://www.ogc.org/standards/3DTiles[3D Tiles]|Both|Y|Y|Entity|Y|Based on glTF

|JSON-FG|Virtual|Y|Y|Attributes (Geometry, Where)|Y|Under Development|
|===

Note: 3D Tiles is basically a binary, encapsulated glTF with georeferencing.

Note: In OMS, where location is foreseen, it is always an ISO19107 Geometry Type. Temporal information is provided both in the Observation and the Deployment, utilizing TM_Object or TM_Period. Temporal attributes are defined in ISO19108.

==== Other SDOs

There are other standards development organizations (SDOs) that deal with location and orientation for graphics. Information on these SDOs is compiled in the tables below. Work done in the W3C defines how systems express location and orientation for browsers. The Motion Imagery Standards Board (MISB) has standards for moving cameras. ISO also has sections of its standards in SC 24, such as the X3D standards, that encode orientation and position in graphics. In the Khronos Group, there are standards such as OpenXR and glTF that specify how to form digital assets that encode position and orientation

__Khronos Group__
[%header,cols="2,2,1,2,2,2"]
|===
|*Standard* |*Graphic/Virtual Context* |*Local SRS* |*Geodedic CRS* |*6DOF as entity or attribute?* |*Temporality*

|link:https://www.khronos.org/gltf/[glTF]
|Both
|Y
|N
|Entity
|Y

|link:https://www.khronos.org/openxr/[OpenXR]
|Virtual
|Y
|N
|Entity
|Y

|===

link:https://www.khronos.org/registry/OpenXR/specs/1.0/html/xrspec.html#XR_MSFT_spatial_anchor[This OpenXR Extension for Microsoft Spatial Anchors] allows an application to create a spatial anchor, an arbitrary freespace point in the user’s physical environment that will then be tracked by the runtime. The runtime should then adjust the position and orientation of that anchor’s origin over time as needed, independently of all other spaces and anchors, to ensure that it maintains its original mapping to the real world.

<<<
__W3C__
[%header,cols="3,2,1,2,2,2,2"]
|===
|*Standard* |*Graphic/Virtual Context* |*Local SRS* |*Geodedic CRS* |*6DOF as entity or attribute?* |*Temporality* |*Remark*

|link:https://w3c.github.io/geolocation-api/[Geolocation API]
|Virtual
|No
|Position & heading only
|Attributes
|Yes
|Working Draft (Nov 21)

|link:https://www.w3.org/TR/orientation-sensor/[Orientation Sensor]
|Virtual
|Orientation only
|Orientation only
|Attributes
|Yes
|Editor's Draft (Nov 21)

|link:https://www.w3.org/TR/webxr/[WebXR Device API]
|Virtual
|Yes
|No
|Entity
|Yes
|Working Draft (Nov 21)
|===

Levels of support for HTML features in current web browsers can be gauged from https://wpt.fyi/results[W3C Web Platform Test results].

From the Immersive Web WebXR Device API documentation: link:https://immersive-web.github.io/webxr/#xrspace-interface[XRSpace] and link:https://immersive-web.github.io/webxr/#pose[XR Pose]
An XRSpace represents a virtual coordinate system with an origin that corresponds to a physical location. Spatial data that is requested from the API or given to the API is always expressed in relation to a specific XRSpace at the time of a specific XRFrame. Numeric values such as pose positions are coordinates in that space relative to its origin. The interface is intentionally opaque.

__Web 3D Consortium__
[%header,cols="3,2,1,2,2,2"]
|===
|*Standard* |*Graphic/Virtual Context* |*Local SRS* |*Geodedic CRS* |*6DOF as entity or attribute?* |*Temporality*
|link:https://doc.x3dom.org/author/Geospatial/GeoTransform.html[X3D Geo Component]
|Both
|Y
|Y
|Attributes
|Y
|===

__Motion Imagery Standards Board (MISB)__
[%header,cols="3,2,2,2,2,2"]
|===
|*Standard* |*Graphic/Virtual Context* |*Local SRS* |*Geodedic CRS* |*6DOF as entity or attribute?* |*Temporality*
|link:https://www.gwg.nga.mil/misb/docs/standards/ST0601.17.pdf[MISB ST 0601]
|Virtual
|Sensor position & orientation relative to platform
|Platform position & orientation
|Attributes
|UTC & media time

|link:https://www.gwg.nga.mil/misb/docs/standards/ST0801.8.pdf[MISB ST 0801]
|Virtual
|No
|Camera position & orientation
|Attributes
|UTC & media time
|===

<<<
__Third Generation Partnership Project (3GPP)__
[%header,cols="3,2,1,2,2,2"]
|===
|*Standard* |*Graphic/Virtual Context* |*Local SRS* |*Geodedic CRS* |*6DOF as entity or attribute?* |*Temporality* |
link:https://portal.3gpp.org/desktopmodules/Specifications/SpecificationDetails.aspx?specificationId=1441[3GP (26.244)]
|Virtual
|No
|Y
|Attributes
|Media time
|===

__Camera & Imaging Products Association (CIPA/JEITA)__
[%header,cols="3,2,1,2,2,2"]
|===
|*Standard* |*Graphic/Virtual Context* |*Local SRS* |*Geodedic CRS* |*6DOF as entity or attribute?* |*Temporality*
|link:https://www.cipa.jp/std/documents/e/DC-X008-Translation-2019-E.pdf[Exif]
|Virtual
|No
|Position & heading only
|Attributes
|UTC
|===


__International Organization for Standardization (ISO)__
[%header,cols="3,2,1,2,2,2"]
|===
|*Standard* |*Graphic/Virtual Context* |*Local SRS* |*Geodedic CRS* |*6DOF as entity or attribute?* |*Temporality*
|link:https://www.iso.org/standard/54166.html[Spatial Reference Model (18026)]
|Virtual
|No
|Position only
|Attributes
|UTC

|link:https://mpeg.chiariglione.org/standards/mpeg-i[MPEG Immersive Video (MIV) MPEG-I (23090)]
|Virtual
|No
|Yes
|Attributes
|Media time


|link:https://www.iso.org/standard/66175.html[ISO19107]
|Virtual
|Y
|Y
|3DOF
|N

|link:https://www.iso.org/standard/26013.html[ISO19108]
|Virtual
|Y
|Y
|N
|Y

|===

__Institute of Electrical and Electronics Engineers (IEEE)__
[%header,cols="3,2,1,2,2,2,2"]
|===
|*Standard* |*Graphic/Virtual Context* |*Local SRS* |*Geodedic CRS* |*6DOF as entity or attribute?* |*Temporality* |*Remark*
|link:https://standards.ieee.org/project/1278_1.html[Distributed Interactive Simulation (1278)]
|Virtual
|No
|Position only
|Attribute
|Y
|

|link:https://standards.ieee.org/content/dam/ieee-standards/standards/web/documents/tutorials/1451d4.pdf[Smart Transducer Interface (1451)]
|N
|Y
|Y
|Entity
|Y
|Based on GML
|===

__BuildingSmart__
[%header,cols="3,2,1,2,2,2"]
|===
|*Standard* |*Graphic/Virtual Context* |*Local SRS* |*Geodedic CRS* |*6DOF as entity or attribute?* |*Temporality*
|link:https://www.buildingsmart.org/standards/bsi-standards/industry-foundation-classes/[Industry Foundation Classes (IFC)]
|Y
|Y
|Y
|No
|No
|===

IfcSite and other IfCProducts permits topologic orientation, but not 6DOF. IFCSite lets users provide the WGS84 location (lat,lng,alt) of  "the single geographic reference point for (http://standards.buildingsmart.org/MVD/RELEASE/IFC4/ADD2_TC1/RV1_2/HTML/schema/ifcproductextension/lexical/ifcsite.htm)[this site])"

For orientation they refer to the concept of "true north": "The world coordinate system, established at the IfcProject.RepresentationContexts, may include a definition of the true north within the XY plane of the world coordinate system, if provided, it can be obtained at IfcGeometricRepresentationContext.TrueNorth."


__ASTM__
[%header,cols="3,2,1,2,2,2"]
|===
|*Standard* |*Graphic/Virtual Context* |*Local SRS* |*Geodedic CRS* |*6DOF as entity or attribute?* |*Temporality*
|link:https://www.astm.org/e2807-11r19.html[E57 3D Imaging Data Exchange (E2807-11 (2019))]
|No
|Y
|Y
|Attribute
|Y
|===

The core capabilities of the E57 data exchange format link:http://libe57.org/features.html[includes fifteen features.]

__Schema.org__
[%header,cols="3,2,1,2,2,2"]
|===
|*Standard* |*Graphic/Virtual Context* |*Local SRS* |*Geodedic CRS* |*6DOF as entity or attribute?* |*Temporality*
|link:https://schema.org/Place[Place]
|No
|N
|Y
|3DOF
|N

|link:https://schema.org/GeoCoordinates[GeoCoordinates]
|No
|N
|Y
|3DOF
|N
|===

==== Space Science
There are also specifications (standards) that are developed for and used by industries/domains.

The Observation Geometry System used by NASA for space science missions is called SPICE. The OGC GeoPose SWG has chosen to make reference particularly to SPICE in this document. SPICE is an elegant system which ties together ephemeris information (including position and orientation data) in contexts ranging from the Earth system through to spacecraft, solar system (with the link:https://en.wikipedia.org/wiki/Earth-centered_inertial['J2000']  fundamental inertial reference system) and planetary bodies. The SWG members were inspired by some of the concepts, particularly the ideas of frame of reference transformations and of satellites, constellations of satellites and other objects in orbit around the Earth. SPICE handles complex situations such as the relative pointing of spacecraft in motion around other bodies in the Solar System - this flexibility points to a complete and elegant solution.

While the SWG appreciated the full treatment of frame transformation in the SPICE system, the OGC SWG members took the approach of reducing the scope to Earth-based systems in version 1 of the GeoPose standard. The intention has been to permit later extension to a wider solar system viewpoint.

.NAIF SPICE
image::NAIF-SPICE.png["NAIF SPICE"  pdfwidth="14cm"]

A tutorial presentation about SPICE is available link:https://naif.jpl.nasa.gov/pub/naif/toolkit_docs/Tutorials/pdf/individual_docs/03_spice_overview.pdf[here].

The link:https://naif.jpl.nasa.gov/pub/naif/toolkit_docs/Tutorials/pdf/individual_docs/21_fk.pdf[Frames Kernel] is the key component of SPICE to link reference frames and which in particular inspired the frame transformations in OGC GeoPose.

__Space Data Standards__

International space data standards are documented in https://ccsds.org[Consultative Committee for Space Data Systems (CCSDS)] Blue Books. Spacecraft position and orientation are described as _attitude_ as described in section 5.3 of https://public.ccsds.org/Pubs/500x0g4.pdf[CCSDS Navigation Data - Definitions and Conventions]. Typical GeoPose use cases include antenna tracking, sun sensor, star sensor, gyro package and horizon sensor.

Beyond those mentioned in this section, there are other link:http://spacedatastandards.org/[space data standards] with which the reviewer may wish to be acquainted.
